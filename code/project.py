# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KvzurVuKddXAHp53MsW2DvvVftDG7PKd
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn import metrics
from sklearn.svm import SVC
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
import warnings
warnings.filterwarnings('ignore')

path = "/content/drive/MyDrive/dataset/exercise.csv"
df = pd.read_csv(path)
df.head()

df.shape

df.info()

df.describe()

sb.scatterplot(data=df, x='Height', y='Weight')
plt.show()

features = ['Age', 'Height', 'Weight', 'Duration']

plt.subplots(figsize=(15, 10))
for i, col in enumerate(features):
    plt.subplot(2, 2, i + 1)
    sb.scatterplot(data=df.sample(1000), x=col, y='Calories')  
    plt.title(f'Scatter Plot: {col} vs Calories')

plt.tight_layout()
plt.show()

numeric_columns = df.select_dtypes(include=['float', 'int']).columns

print("Selected Numeric Columns:", numeric_columns)

plt.figure(figsize=(15, 10))
for i, col in enumerate(numeric_columns):
    plt.subplot(2, 4, i + 1)
    sb.distplot(df[col].dropna())
    plt.title(f'Distribution Plot: {col}')

plt.tight_layout()
plt.show()

df.replace({'male': 0, 'female': 1},
           inplace=True)
df.head()

plt.figure(figsize=(8, 8))
sb.heatmap(df.corr() > 0.9,
           annot=True,
           cbar=False)
plt.show()

to_remove = ['Weight', 'Duration']
df.drop(to_remove, axis=1, inplace=True)

features = df.drop(['User_ID', 'Calories'], axis=1)
target = df['Calories'].values

X_train, X_val, Y_train, Y_val = train_test_split(features, target,
                                                  test_size=0.1,
                                                  random_state=22)

print(X_train.shape, X_val.shape)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
models = [
    LinearRegression(),
    DecisionTreeRegressor(),
    RandomForestRegressor(),
    AdaBoostRegressor(),
    GradientBoostingRegressor(),
    XGBRegressor()
]

for model in models:
    model.fit(X_train, Y_train)

    print(f'{model} : ')

    train_preds = model.predict(X_train)
    val_preds = model.predict(X_val)

    mse_train = mean_squared_error(Y_train, train_preds)
    rmse_train = mean_squared_error(Y_train, train_preds, squared=False)
    r2_train = r2_score(Y_train, train_preds)
    mae_train = mean_absolute_error(Y_train, train_preds)

    mse_val = mean_squared_error(Y_val, val_preds)
    rmse_val = mean_squared_error(Y_val, val_preds, squared=False)
    r2_val = r2_score(Y_val, val_preds)
    mae_val = mean_absolute_error(Y_val, val_preds)

    print('Training Metrics:')
    print(f'Mean Squared Error: {mse_train}')
    print(f'Root Mean Squared Error: {rmse_train}')
    print(f'R-squared: {r2_train}')
    print(f'Mean Absolute Error: {mae_train}')

    print('\nValidation Metrics:')
    print(f'Mean Squared Error: {mse_val}')
    print(f'Root Mean Squared Error: {rmse_val}')
    print(f'R-squared: {r2_val}')
    print(f'Mean Absolute Error: {mae_val}')

    print()

X_for_clustering = df[['Age', 'Duration']]


kmeans = KMeans(n_clusters=3, random_state=42)
df['Cluster_Label'] = kmeans.fit_predict(X_for_clustering)


X_with_clusters = df.drop(['Calories'], axis=1)
y = df['Calories']


gender_column = 'Gender'
if gender_column in X_with_clusters.columns:

    preprocessor = ColumnTransformer(
        transformers=[
            ('encoder', OneHotEncoder(), [gender_column])
        ],
        remainder='passthrough'
    )


    X_transformed = preprocessor.fit_transform(X_with_clusters)

    kfolds = [2, 10]

    for kfold in kfolds:

        kf = KFold(n_splits=kfold, shuffle=True, random_state=42)

        for model in models:

            scores = cross_val_score(model, X_transformed, y, cv=kf, scoring='neg_mean_squared_error') 

            print(f'{model} with {kfold}-fold cross-validation:')
            print(f'Average MSE: {-np.mean(scores)}')  
            print()
else:
    print(f"The '{gender_column}' column is not present in the DataFrame.")

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import KFold, cross_val_predict
import numpy as np



y = df['Calories']


kf = KFold(n_splits=10, shuffle=True, random_state=42)


models = [
    LinearRegression(),
    DecisionTreeRegressor(),
    RandomForestRegressor(),
    AdaBoostRegressor(),
    GradientBoostingRegressor(),
    XGBRegressor()
]


for model in models:
    print(f'Model: {model}')


    predicted_labels = cross_val_predict(model, X_transformed, y, cv=kf)

    mse = mean_squared_error(y, predicted_labels)
    rmse = mean_squared_error(y, predicted_labels, squared=False)
    r2 = r2_score(y, predicted_labels)
    mae = mean_absolute_error(y, predicted_labels)

    print(f'Mean Squared Error: {mse}')
    print(f'Root Mean Squared Error: {rmse}')
    print(f'R-squared: {r2}')
    print(f'Mean Absolute Error: {mae}\n')

